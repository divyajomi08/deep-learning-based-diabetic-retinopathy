{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-08T12:05:25.246994Z","iopub.execute_input":"2021-06-08T12:05:25.247386Z","iopub.status.idle":"2021-06-08T12:05:25.258301Z","shell.execute_reply.started":"2021-06-08T12:05:25.247292Z","shell.execute_reply":"2021-06-08T12:05:25.256853Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport os\nimport matplotlib.pyplot as plt\nimport PIL\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom plotly.offline import iplot, init_notebook_mode\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.utils import plot_model\nfrom IPython.display import display\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:04.828014Z","iopub.execute_input":"2021-06-08T12:07:04.828352Z","iopub.status.idle":"2021-06-08T12:07:11.668558Z","shell.execute_reply.started":"2021-06-08T12:07:04.828323Z","shell.execute_reply":"2021-06-08T12:07:11.667412Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/aptospreprocessed/preprocess/'","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:11.670398Z","iopub.execute_input":"2021-06-08T12:07:11.670847Z","iopub.status.idle":"2021-06-08T12:07:11.675869Z","shell.execute_reply.started":"2021-06-08T12:07:11.670779Z","shell.execute_reply":"2021-06-08T12:07:11.674570Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_names = []\ntrain_labels = []\n\n# for i in os.listdir(os.path.join('/kaggle/input/idrid-dataset-cropped', 'dataset')):\nfor i in range(5):\n    train_class = os.listdir(os.path.join('/kaggle/input/aptospreprocessed/preprocess', str(i)))\n    for j in train_class:\n        img = os.path.join(str(i), j)\n        train_names.append(img)\n        if str(i) in ['2', '3', '4']:\n            train_labels.append('1')\n        else:\n            train_labels.append(str(i))\nprint('Number of train images : {} ({})\\n'.format(len(train_names), len(train_labels)))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:14.733556Z","iopub.execute_input":"2021-06-08T12:07:14.733942Z","iopub.status.idle":"2021-06-08T12:07:16.127783Z","shell.execute_reply.started":"2021-06-08T12:07:14.733912Z","shell.execute_reply":"2021-06-08T12:07:16.126735Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of train images : 3662 (3662)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"sns.countplot(x=train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:20.105326Z","iopub.execute_input":"2021-06-08T12:07:20.105889Z","iopub.status.idle":"2021-06-08T12:07:20.415562Z","shell.execute_reply.started":"2021-06-08T12:07:20.105838Z","shell.execute_reply":"2021-06-08T12:07:20.414475Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:ylabel='count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"fig, axs = plt.subplots(5, 5, figsize = (20, 20))\ncount = 0\n# for i in os.listdir(os.path.join('/kaggle/input/idrid','train-crop-classwise')):\n# for i in os.listdir(os.path.join('/kaggle/input/aptosdatasetcropped', 'dataset-aptos')):\n    # get the list of images in a given class\n    # try:\n    #    train_class = os.listdir(os.path.join('/kaggle/input/aptosdatasetcropped/dataset-aptos', i))\n    # except NotADirectoryError:\n    #    pass\n  # plot 5 images per class\n\ntrain_class = ['0', '1', '2', '3', '4']\nfor j in range(5):\n    take_img = '/kaggle/input/aptosdatasetcropped/dataset-aptos/' + train_class[j] + '/'\n    \n    arr = os.listdir(take_img)[:5]\n    # print(arr)\n    for i in range(len(arr)):\n    # for name in arr:\n        name = arr[i]\n        img = PIL.Image.open(take_img + name)\n        axs[count][i].title.set_text(j)\n        axs[count][i].imshow(img)\n    count += 1\n\"\"\"   \nfor j in range(5):\n    take_img = '/kaggle/input/aptosdatasetcropped/dataset-aptos'\n    img = os.path.join('/kaggle/input/aptosdatasetcropped/dataset-aptos', i, train_class[j])\n    img = PIL.Image.open(img)\n    axs[count][j].title.set_text(i)\n    axs[count][j].imshow(img)  \ncount += 1\n\"\"\"\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T13:49:56.479741Z","iopub.execute_input":"2021-06-06T13:49:56.480095Z","iopub.status.idle":"2021-06-06T13:50:00.760539Z","shell.execute_reply.started":"2021-06-06T13:49:56.480063Z","shell.execute_reply":"2021-06-06T13:50:00.759777Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retina_df = pd.DataFrame({'Image': train_names, 'Labels': train_labels})\nretina_df = shuffle(retina_df)\nretina_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:24.571826Z","iopub.execute_input":"2021-06-08T12:07:24.572228Z","iopub.status.idle":"2021-06-08T12:07:24.748881Z","shell.execute_reply.started":"2021-06-08T12:07:24.572195Z","shell.execute_reply":"2021-06-08T12:07:24.747243Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                    Image Labels\n1089  0/d6df4fe492ec.jpeg      0\n94    0/1ec95179cdfe.jpeg      0\n146   0/976082127e2a.jpeg      0\n400   0/c06024f05a16.jpeg      0\n1483  0/691eeb59b4cb.jpeg      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1089</th>\n      <td>0/d6df4fe492ec.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>0/1ec95179cdfe.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0/976082127e2a.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>400</th>\n      <td>0/c06024f05a16.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1483</th>\n      <td>0/691eeb59b4cb.jpeg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train, test = train_test_split(retina_df, test_size = 0.2)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:28.035080Z","iopub.execute_input":"2021-06-08T12:07:28.035580Z","iopub.status.idle":"2021-06-08T12:07:28.057998Z","shell.execute_reply.started":"2021-06-08T12:07:28.035537Z","shell.execute_reply":"2021-06-08T12:07:28.056684Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                    Image Labels\n3533  4/61e301bd3c25.jpeg      1\n25    0/342edf9b889d.jpeg      0\n2121  1/ca1036496659.jpeg      1\n918   0/bb2f89488ecd.jpeg      0\n2260  2/47b756014447.jpeg      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3533</th>\n      <td>4/61e301bd3c25.jpeg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0/342edf9b889d.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2121</th>\n      <td>1/ca1036496659.jpeg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>918</th>\n      <td>0/bb2f89488ecd.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2260</th>\n      <td>2/47b756014447.jpeg</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"one = retina_df['Labels'].value_counts()\none","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:07:29.581063Z","iopub.execute_input":"2021-06-08T14:07:29.581411Z","iopub.status.idle":"2021-06-08T14:07:29.595979Z","shell.execute_reply.started":"2021-06-08T14:07:29.581381Z","shell.execute_reply":"2021-06-08T14:07:29.594412Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"1    1857\n0    1805\nName: Labels, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:30.459998Z","iopub.execute_input":"2021-06-08T12:07:30.460393Z","iopub.status.idle":"2021-06-08T12:07:30.473439Z","shell.execute_reply.started":"2021-06-08T12:07:30.460349Z","shell.execute_reply":"2021-06-08T12:07:30.472240Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                    Image Labels\n280   0/3599029efeb3.jpeg      0\n276   0/4a1afe4044f4.jpeg      0\n2459  2/39923b29988a.jpeg      1\n308   0/afc345cc9145.jpeg      0\n202   0/d5c63a8d9e94.jpeg      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>280</th>\n      <td>0/3599029efeb3.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>276</th>\n      <td>0/4a1afe4044f4.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2459</th>\n      <td>2/39923b29988a.jpeg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>308</th>\n      <td>0/afc345cc9145.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>0/d5c63a8d9e94.jpeg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n        rescale = 1./255,\n        validation_split = 0.15)\n\ntest_datagen = ImageDataGenerator(rescale = 1./255)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:34.946310Z","iopub.execute_input":"2021-06-08T12:07:34.946662Z","iopub.status.idle":"2021-06-08T12:07:34.953821Z","shell.execute_reply.started":"2021-06-08T12:07:34.946632Z","shell.execute_reply":"2021-06-08T12:07:34.952626Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_dataframe(\n    train,\n    # directory='/kaggle/input/drdiffclasscrop/train-crop-classwise/',\n    directory=path,\n    x_col=\"Image\",\n    y_col=\"Labels\",\n    target_size=(512, 512),\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=32,\n    subset='training')\n\nvalidation_generator = train_datagen.flow_from_dataframe(\n    train,\n    # directory='/kaggle/input/drdiffclasscrop/train-crop-classwise/',\n    directory=path,\n    x_col=\"Image\",\n    y_col=\"Labels\",\n    target_size=(512, 512),\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=32,\n    subset='validation')\n\ntest_generator = test_datagen.flow_from_dataframe(\n    test,\n    # directory='/kaggle/input/drdiffclasscrop/train-crop-classwise/',\n    directory=path,\n    x_col=\"Image\",\n    y_col=\"Labels\",\n    target_size=(512, 512),\n    color_mode=\"rgb\",\n    class_mode=\"categorical\",\n    batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:37.863745Z","iopub.execute_input":"2021-06-08T12:07:37.864117Z","iopub.status.idle":"2021-06-08T12:07:42.064382Z","shell.execute_reply.started":"2021-06-08T12:07:37.864087Z","shell.execute_reply":"2021-06-08T12:07:42.063121Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Found 2490 validated image filenames belonging to 2 classes.\nFound 439 validated image filenames belonging to 2 classes.\nFound 733 validated image filenames belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"def res_block(X, filter, stage):\n\n    # Convolutional block\n    X_copy = X\n    f1, f2, f3 = filter\n\n    # Main Path\n    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = MaxPool2D((2,2))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)\n\n    # Short Path\n    X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)\n    X_copy = MaxPool2D((2,2))(X_copy)\n    X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)\n\n    # ADD\n    X = Add()([X,X_copy])\n    X = Activation('relu')(X)\n\n    # Identity Block 1\n    X_copy = X\n\n\n    # Main Path\n    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(X)\n\n    # ADD\n    X = Add()([X,X_copy])\n    X = Activation('relu')(X)\n\n    # Identity Block 2\n    X_copy = X\n\n\n    # Main Path\n    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)\n\n    # ADD\n    X = Add()([X,X_copy])\n    X = Activation('relu')(X)\n\n    return X\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:42.069271Z","iopub.execute_input":"2021-06-08T12:07:42.069933Z","iopub.status.idle":"2021-06-08T12:07:42.124364Z","shell.execute_reply.started":"2021-06-08T12:07:42.069884Z","shell.execute_reply":"2021-06-08T12:07:42.123124Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"input_shape = (512,512,3)\n\n#Input tensor shape\nX_input = Input(input_shape)\n\n#Zero-padding\n\nX = ZeroPadding2D((3,3))(X_input)\n\n# 1 - stage\n\nX = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\nX = BatchNormalization(axis =3, name = 'bn_conv1')(X)\nX = Activation('relu')(X)\nX = MaxPooling2D((3,3), strides= (2,2))(X)\n\n# 2- stage\n\nX = res_block(X, filter= [64,64,256], stage= 2)\n\n# 3- stage\n\nX = res_block(X, filter= [128,128,512], stage= 3)\n\n# 4- stage\n\nX = res_block(X, filter= [256,256,1024], stage= 4)\n\n# # 5- stage\n\nX = res_block(X, filter= [512,512,2048], stage= 5)\n\n#Average Pooling\n\nX = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)\n\n#Final layer\n\nX = Flatten()(X)\n# X = Dense(5, activation = 'softmax', name = 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)\nX = Dense(2, activation = 'softmax', name = 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)\n\nmodel = Model( inputs= X_input, outputs = X, name = 'Resnet18')\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:44.047665Z","iopub.execute_input":"2021-06-08T12:07:44.048226Z","iopub.status.idle":"2021-06-08T12:07:47.419348Z","shell.execute_reply.started":"2021-06-08T12:07:44.048193Z","shell.execute_reply":"2021-06-08T12:07:47.418338Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"Resnet18\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 512, 512, 3) 0                                            \n__________________________________________________________________________________________________\nzero_padding2d (ZeroPadding2D)  (None, 518, 518, 3)  0           input_1[0][0]                    \n__________________________________________________________________________________________________\nconv1 (Conv2D)                  (None, 256, 256, 64) 9472        zero_padding2d[0][0]             \n__________________________________________________________________________________________________\nbn_conv1 (BatchNormalization)   (None, 256, 256, 64) 256         conv1[0][0]                      \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 256, 256, 64) 0           bn_conv1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 127, 127, 64) 0           activation[0][0]                 \n__________________________________________________________________________________________________\nres_2_conv_a (Conv2D)           (None, 127, 127, 64) 4160        max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 63, 63, 64)   0           res_2_conv_a[0][0]               \n__________________________________________________________________________________________________\nbn_2_conv_a (BatchNormalization (None, 63, 63, 64)   256         max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 63, 63, 64)   0           bn_2_conv_a[0][0]                \n__________________________________________________________________________________________________\nres_2_conv_b (Conv2D)           (None, 63, 63, 64)   36928       activation_1[0][0]               \n__________________________________________________________________________________________________\nbn_2_conv_b (BatchNormalization (None, 63, 63, 64)   256         res_2_conv_b[0][0]               \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 63, 63, 64)   0           bn_2_conv_b[0][0]                \n__________________________________________________________________________________________________\nres_2_conv_copy (Conv2D)        (None, 127, 127, 256 16640       max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nres_2_conv_c (Conv2D)           (None, 63, 63, 256)  16640       activation_2[0][0]               \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 63, 63, 256)  0           res_2_conv_copy[0][0]            \n__________________________________________________________________________________________________\nbn_2_conv_c (BatchNormalization (None, 63, 63, 256)  1024        res_2_conv_c[0][0]               \n__________________________________________________________________________________________________\nbn_2_conv_copy (BatchNormalizat (None, 63, 63, 256)  1024        max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nadd (Add)                       (None, 63, 63, 256)  0           bn_2_conv_c[0][0]                \n                                                                 bn_2_conv_copy[0][0]             \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 63, 63, 256)  0           add[0][0]                        \n__________________________________________________________________________________________________\nres_2_identity_1_a (Conv2D)     (None, 63, 63, 64)   16448       activation_3[0][0]               \n__________________________________________________________________________________________________\nbn_2_identity_1_a (BatchNormali (None, 63, 63, 64)   256         res_2_identity_1_a[0][0]         \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, 63, 63, 64)   0           bn_2_identity_1_a[0][0]          \n__________________________________________________________________________________________________\nres_2_identity_1_b (Conv2D)     (None, 63, 63, 64)   36928       activation_4[0][0]               \n__________________________________________________________________________________________________\nbn_2_identity_1_b (BatchNormali (None, 63, 63, 64)   256         res_2_identity_1_b[0][0]         \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 63, 63, 64)   0           bn_2_identity_1_b[0][0]          \n__________________________________________________________________________________________________\nres_2_identity_1_c (Conv2D)     (None, 63, 63, 256)  16640       activation_5[0][0]               \n__________________________________________________________________________________________________\nbn_2_identity_1_c (BatchNormali (None, 63, 63, 256)  1024        res_2_identity_1_c[0][0]         \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 63, 63, 256)  0           bn_2_identity_1_c[0][0]          \n                                                                 activation_3[0][0]               \n__________________________________________________________________________________________________\nactivation_6 (Activation)       (None, 63, 63, 256)  0           add_1[0][0]                      \n__________________________________________________________________________________________________\nres_2_identity_2_a (Conv2D)     (None, 63, 63, 64)   16448       activation_6[0][0]               \n__________________________________________________________________________________________________\nbn_2_identity_2_a (BatchNormali (None, 63, 63, 64)   256         res_2_identity_2_a[0][0]         \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 63, 63, 64)   0           bn_2_identity_2_a[0][0]          \n__________________________________________________________________________________________________\nres_2_identity_2_b (Conv2D)     (None, 63, 63, 64)   36928       activation_7[0][0]               \n__________________________________________________________________________________________________\nbn_2_identity_2_b (BatchNormali (None, 63, 63, 64)   256         res_2_identity_2_b[0][0]         \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 63, 63, 64)   0           bn_2_identity_2_b[0][0]          \n__________________________________________________________________________________________________\nres_2_identity_2_c (Conv2D)     (None, 63, 63, 256)  16640       activation_8[0][0]               \n__________________________________________________________________________________________________\nbn_2_identity_2_c (BatchNormali (None, 63, 63, 256)  1024        res_2_identity_2_c[0][0]         \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 63, 63, 256)  0           bn_2_identity_2_c[0][0]          \n                                                                 activation_6[0][0]               \n__________________________________________________________________________________________________\nactivation_9 (Activation)       (None, 63, 63, 256)  0           add_2[0][0]                      \n__________________________________________________________________________________________________\nres_3_conv_a (Conv2D)           (None, 63, 63, 128)  32896       activation_9[0][0]               \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 31, 31, 128)  0           res_3_conv_a[0][0]               \n__________________________________________________________________________________________________\nbn_3_conv_a (BatchNormalization (None, 31, 31, 128)  512         max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nactivation_10 (Activation)      (None, 31, 31, 128)  0           bn_3_conv_a[0][0]                \n__________________________________________________________________________________________________\nres_3_conv_b (Conv2D)           (None, 31, 31, 128)  147584      activation_10[0][0]              \n__________________________________________________________________________________________________\nbn_3_conv_b (BatchNormalization (None, 31, 31, 128)  512         res_3_conv_b[0][0]               \n__________________________________________________________________________________________________\nactivation_11 (Activation)      (None, 31, 31, 128)  0           bn_3_conv_b[0][0]                \n__________________________________________________________________________________________________\nres_3_conv_copy (Conv2D)        (None, 63, 63, 512)  131584      activation_9[0][0]               \n__________________________________________________________________________________________________\nres_3_conv_c (Conv2D)           (None, 31, 31, 512)  66048       activation_11[0][0]              \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 31, 31, 512)  0           res_3_conv_copy[0][0]            \n__________________________________________________________________________________________________\nbn_3_conv_c (BatchNormalization (None, 31, 31, 512)  2048        res_3_conv_c[0][0]               \n__________________________________________________________________________________________________\nbn_3_conv_copy (BatchNormalizat (None, 31, 31, 512)  2048        max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 31, 31, 512)  0           bn_3_conv_c[0][0]                \n                                                                 bn_3_conv_copy[0][0]             \n__________________________________________________________________________________________________\nactivation_12 (Activation)      (None, 31, 31, 512)  0           add_3[0][0]                      \n__________________________________________________________________________________________________\nres_3_identity_1_a (Conv2D)     (None, 31, 31, 128)  65664       activation_12[0][0]              \n__________________________________________________________________________________________________\nbn_3_identity_1_a (BatchNormali (None, 31, 31, 128)  512         res_3_identity_1_a[0][0]         \n__________________________________________________________________________________________________\nactivation_13 (Activation)      (None, 31, 31, 128)  0           bn_3_identity_1_a[0][0]          \n__________________________________________________________________________________________________\nres_3_identity_1_b (Conv2D)     (None, 31, 31, 128)  147584      activation_13[0][0]              \n__________________________________________________________________________________________________\nbn_3_identity_1_b (BatchNormali (None, 31, 31, 128)  512         res_3_identity_1_b[0][0]         \n__________________________________________________________________________________________________\nactivation_14 (Activation)      (None, 31, 31, 128)  0           bn_3_identity_1_b[0][0]          \n__________________________________________________________________________________________________\nres_3_identity_1_c (Conv2D)     (None, 31, 31, 512)  66048       activation_14[0][0]              \n__________________________________________________________________________________________________\nbn_3_identity_1_c (BatchNormali (None, 31, 31, 512)  2048        res_3_identity_1_c[0][0]         \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 31, 31, 512)  0           bn_3_identity_1_c[0][0]          \n                                                                 activation_12[0][0]              \n__________________________________________________________________________________________________\nactivation_15 (Activation)      (None, 31, 31, 512)  0           add_4[0][0]                      \n__________________________________________________________________________________________________\nres_3_identity_2_a (Conv2D)     (None, 31, 31, 128)  65664       activation_15[0][0]              \n__________________________________________________________________________________________________\nbn_3_identity_2_a (BatchNormali (None, 31, 31, 128)  512         res_3_identity_2_a[0][0]         \n__________________________________________________________________________________________________\nactivation_16 (Activation)      (None, 31, 31, 128)  0           bn_3_identity_2_a[0][0]          \n__________________________________________________________________________________________________\nres_3_identity_2_b (Conv2D)     (None, 31, 31, 128)  147584      activation_16[0][0]              \n__________________________________________________________________________________________________\nbn_3_identity_2_b (BatchNormali (None, 31, 31, 128)  512         res_3_identity_2_b[0][0]         \n__________________________________________________________________________________________________\nactivation_17 (Activation)      (None, 31, 31, 128)  0           bn_3_identity_2_b[0][0]          \n__________________________________________________________________________________________________\nres_3_identity_2_c (Conv2D)     (None, 31, 31, 512)  66048       activation_17[0][0]              \n__________________________________________________________________________________________________\nbn_3_identity_2_c (BatchNormali (None, 31, 31, 512)  2048        res_3_identity_2_c[0][0]         \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 31, 31, 512)  0           bn_3_identity_2_c[0][0]          \n                                                                 activation_15[0][0]              \n__________________________________________________________________________________________________\nactivation_18 (Activation)      (None, 31, 31, 512)  0           add_5[0][0]                      \n__________________________________________________________________________________________________\nres_4_conv_a (Conv2D)           (None, 31, 31, 256)  131328      activation_18[0][0]              \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 15, 15, 256)  0           res_4_conv_a[0][0]               \n__________________________________________________________________________________________________\nbn_4_conv_a (BatchNormalization (None, 15, 15, 256)  1024        max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nactivation_19 (Activation)      (None, 15, 15, 256)  0           bn_4_conv_a[0][0]                \n__________________________________________________________________________________________________\nres_4_conv_b (Conv2D)           (None, 15, 15, 256)  590080      activation_19[0][0]              \n__________________________________________________________________________________________________\nbn_4_conv_b (BatchNormalization (None, 15, 15, 256)  1024        res_4_conv_b[0][0]               \n__________________________________________________________________________________________________\nactivation_20 (Activation)      (None, 15, 15, 256)  0           bn_4_conv_b[0][0]                \n__________________________________________________________________________________________________\nres_4_conv_copy (Conv2D)        (None, 31, 31, 1024) 525312      activation_18[0][0]              \n__________________________________________________________________________________________________\nres_4_conv_c (Conv2D)           (None, 15, 15, 1024) 263168      activation_20[0][0]              \n__________________________________________________________________________________________________\nmax_pooling2d_6 (MaxPooling2D)  (None, 15, 15, 1024) 0           res_4_conv_copy[0][0]            \n__________________________________________________________________________________________________\nbn_4_conv_c (BatchNormalization (None, 15, 15, 1024) 4096        res_4_conv_c[0][0]               \n__________________________________________________________________________________________________\nbn_4_conv_copy (BatchNormalizat (None, 15, 15, 1024) 4096        max_pooling2d_6[0][0]            \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 15, 15, 1024) 0           bn_4_conv_c[0][0]                \n                                                                 bn_4_conv_copy[0][0]             \n__________________________________________________________________________________________________\nactivation_21 (Activation)      (None, 15, 15, 1024) 0           add_6[0][0]                      \n__________________________________________________________________________________________________\nres_4_identity_1_a (Conv2D)     (None, 15, 15, 256)  262400      activation_21[0][0]              \n__________________________________________________________________________________________________\nbn_4_identity_1_a (BatchNormali (None, 15, 15, 256)  1024        res_4_identity_1_a[0][0]         \n__________________________________________________________________________________________________\nactivation_22 (Activation)      (None, 15, 15, 256)  0           bn_4_identity_1_a[0][0]          \n__________________________________________________________________________________________________\nres_4_identity_1_b (Conv2D)     (None, 15, 15, 256)  590080      activation_22[0][0]              \n__________________________________________________________________________________________________\nbn_4_identity_1_b (BatchNormali (None, 15, 15, 256)  1024        res_4_identity_1_b[0][0]         \n__________________________________________________________________________________________________\nactivation_23 (Activation)      (None, 15, 15, 256)  0           bn_4_identity_1_b[0][0]          \n__________________________________________________________________________________________________\nres_4_identity_1_c (Conv2D)     (None, 15, 15, 1024) 263168      activation_23[0][0]              \n__________________________________________________________________________________________________\nbn_4_identity_1_c (BatchNormali (None, 15, 15, 1024) 4096        res_4_identity_1_c[0][0]         \n__________________________________________________________________________________________________\nadd_7 (Add)                     (None, 15, 15, 1024) 0           bn_4_identity_1_c[0][0]          \n                                                                 activation_21[0][0]              \n__________________________________________________________________________________________________\nactivation_24 (Activation)      (None, 15, 15, 1024) 0           add_7[0][0]                      \n__________________________________________________________________________________________________\nres_4_identity_2_a (Conv2D)     (None, 15, 15, 256)  262400      activation_24[0][0]              \n__________________________________________________________________________________________________\nbn_4_identity_2_a (BatchNormali (None, 15, 15, 256)  1024        res_4_identity_2_a[0][0]         \n__________________________________________________________________________________________________\nactivation_25 (Activation)      (None, 15, 15, 256)  0           bn_4_identity_2_a[0][0]          \n__________________________________________________________________________________________________\nres_4_identity_2_b (Conv2D)     (None, 15, 15, 256)  590080      activation_25[0][0]              \n__________________________________________________________________________________________________\nbn_4_identity_2_b (BatchNormali (None, 15, 15, 256)  1024        res_4_identity_2_b[0][0]         \n__________________________________________________________________________________________________\nactivation_26 (Activation)      (None, 15, 15, 256)  0           bn_4_identity_2_b[0][0]          \n__________________________________________________________________________________________________\nres_4_identity_2_c (Conv2D)     (None, 15, 15, 1024) 263168      activation_26[0][0]              \n__________________________________________________________________________________________________\nbn_4_identity_2_c (BatchNormali (None, 15, 15, 1024) 4096        res_4_identity_2_c[0][0]         \n__________________________________________________________________________________________________\nadd_8 (Add)                     (None, 15, 15, 1024) 0           bn_4_identity_2_c[0][0]          \n                                                                 activation_24[0][0]              \n__________________________________________________________________________________________________\nactivation_27 (Activation)      (None, 15, 15, 1024) 0           add_8[0][0]                      \n__________________________________________________________________________________________________\nres_5_conv_a (Conv2D)           (None, 15, 15, 512)  524800      activation_27[0][0]              \n__________________________________________________________________________________________________\nmax_pooling2d_7 (MaxPooling2D)  (None, 7, 7, 512)    0           res_5_conv_a[0][0]               \n__________________________________________________________________________________________________\nbn_5_conv_a (BatchNormalization (None, 7, 7, 512)    2048        max_pooling2d_7[0][0]            \n__________________________________________________________________________________________________\nactivation_28 (Activation)      (None, 7, 7, 512)    0           bn_5_conv_a[0][0]                \n__________________________________________________________________________________________________\nres_5_conv_b (Conv2D)           (None, 7, 7, 512)    2359808     activation_28[0][0]              \n__________________________________________________________________________________________________\nbn_5_conv_b (BatchNormalization (None, 7, 7, 512)    2048        res_5_conv_b[0][0]               \n__________________________________________________________________________________________________\nactivation_29 (Activation)      (None, 7, 7, 512)    0           bn_5_conv_b[0][0]                \n__________________________________________________________________________________________________\nres_5_conv_copy (Conv2D)        (None, 15, 15, 2048) 2099200     activation_27[0][0]              \n__________________________________________________________________________________________________\nres_5_conv_c (Conv2D)           (None, 7, 7, 2048)   1050624     activation_29[0][0]              \n__________________________________________________________________________________________________\nmax_pooling2d_8 (MaxPooling2D)  (None, 7, 7, 2048)   0           res_5_conv_copy[0][0]            \n__________________________________________________________________________________________________\nbn_5_conv_c (BatchNormalization (None, 7, 7, 2048)   8192        res_5_conv_c[0][0]               \n__________________________________________________________________________________________________\nbn_5_conv_copy (BatchNormalizat (None, 7, 7, 2048)   8192        max_pooling2d_8[0][0]            \n__________________________________________________________________________________________________\nadd_9 (Add)                     (None, 7, 7, 2048)   0           bn_5_conv_c[0][0]                \n                                                                 bn_5_conv_copy[0][0]             \n__________________________________________________________________________________________________\nactivation_30 (Activation)      (None, 7, 7, 2048)   0           add_9[0][0]                      \n__________________________________________________________________________________________________\nres_5_identity_1_a (Conv2D)     (None, 7, 7, 512)    1049088     activation_30[0][0]              \n__________________________________________________________________________________________________\nbn_5_identity_1_a (BatchNormali (None, 7, 7, 512)    2048        res_5_identity_1_a[0][0]         \n__________________________________________________________________________________________________\nactivation_31 (Activation)      (None, 7, 7, 512)    0           bn_5_identity_1_a[0][0]          \n__________________________________________________________________________________________________\nres_5_identity_1_b (Conv2D)     (None, 7, 7, 512)    2359808     activation_31[0][0]              \n__________________________________________________________________________________________________\nbn_5_identity_1_b (BatchNormali (None, 7, 7, 512)    2048        res_5_identity_1_b[0][0]         \n__________________________________________________________________________________________________\nactivation_32 (Activation)      (None, 7, 7, 512)    0           bn_5_identity_1_b[0][0]          \n__________________________________________________________________________________________________\nres_5_identity_1_c (Conv2D)     (None, 7, 7, 2048)   1050624     activation_32[0][0]              \n__________________________________________________________________________________________________\nbn_5_identity_1_c (BatchNormali (None, 7, 7, 2048)   8192        res_5_identity_1_c[0][0]         \n__________________________________________________________________________________________________\nadd_10 (Add)                    (None, 7, 7, 2048)   0           bn_5_identity_1_c[0][0]          \n                                                                 activation_30[0][0]              \n__________________________________________________________________________________________________\nactivation_33 (Activation)      (None, 7, 7, 2048)   0           add_10[0][0]                     \n__________________________________________________________________________________________________\nres_5_identity_2_a (Conv2D)     (None, 7, 7, 512)    1049088     activation_33[0][0]              \n__________________________________________________________________________________________________\nbn_5_identity_2_a (BatchNormali (None, 7, 7, 512)    2048        res_5_identity_2_a[0][0]         \n__________________________________________________________________________________________________\nactivation_34 (Activation)      (None, 7, 7, 512)    0           bn_5_identity_2_a[0][0]          \n__________________________________________________________________________________________________\nres_5_identity_2_b (Conv2D)     (None, 7, 7, 512)    2359808     activation_34[0][0]              \n__________________________________________________________________________________________________\nbn_5_identity_2_b (BatchNormali (None, 7, 7, 512)    2048        res_5_identity_2_b[0][0]         \n__________________________________________________________________________________________________\nactivation_35 (Activation)      (None, 7, 7, 512)    0           bn_5_identity_2_b[0][0]          \n__________________________________________________________________________________________________\nres_5_identity_2_c (Conv2D)     (None, 7, 7, 2048)   1050624     activation_35[0][0]              \n__________________________________________________________________________________________________\nbn_5_identity_2_c (BatchNormali (None, 7, 7, 2048)   8192        res_5_identity_2_c[0][0]         \n__________________________________________________________________________________________________\nadd_11 (Add)                    (None, 7, 7, 2048)   0           bn_5_identity_2_c[0][0]          \n                                                                 activation_33[0][0]              \n__________________________________________________________________________________________________\nactivation_36 (Activation)      (None, 7, 7, 2048)   0           add_11[0][0]                     \n__________________________________________________________________________________________________\nAveragea_Pooling (AveragePoolin (None, 3, 3, 2048)   0           activation_36[0][0]              \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 18432)        0           Averagea_Pooling[0][0]           \n__________________________________________________________________________________________________\nDense_final (Dense)             (None, 2)            36866       flatten[0][0]                    \n==================================================================================================\nTotal params: 19,976,834\nTrainable params: 19,934,466\nNon-trainable params: 42,368\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:54.578564Z","iopub.execute_input":"2021-06-08T12:07:54.579112Z","iopub.status.idle":"2021-06-08T12:07:54.601748Z","shell.execute_reply.started":"2021-06-08T12:07:54.579078Z","shell.execute_reply":"2021-06-08T12:07:54.600271Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"checkpointer = ModelCheckpoint(filepath=\"/kaggle/working/preprocess-binary-aptos-resnet-5-100-weights.hdf5\", verbose=1, save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:07:59.136853Z","iopub.execute_input":"2021-06-08T12:07:59.137240Z","iopub.status.idle":"2021-06-08T12:07:59.141727Z","shell.execute_reply.started":"2021-06-08T12:07:59.137209Z","shell.execute_reply":"2021-06-08T12:07:59.140627Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, steps_per_epoch = train_generator.n // 32, epochs = 100, validation_data= validation_generator, validation_steps= validation_generator.n // 32, callbacks=[checkpointer])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:39:26.302130Z","iopub.execute_input":"2021-06-08T12:39:26.302578Z","iopub.status.idle":"2021-06-08T13:47:48.977249Z","shell.execute_reply.started":"2021-06-08T12:39:26.302524Z","shell.execute_reply":"2021-06-08T13:47:48.976067Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/100\n77/77 [==============================] - 40s 518ms/step - loss: 0.0088 - accuracy: 0.9959 - val_loss: 0.1985 - val_accuracy: 0.9615\n\nEpoch 00001: val_loss did not improve from 0.13747\nEpoch 2/100\n77/77 [==============================] - 40s 519ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 0.2235 - val_accuracy: 0.9615\n\nEpoch 00002: val_loss did not improve from 0.13747\nEpoch 3/100\n77/77 [==============================] - 40s 522ms/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 0.3098 - val_accuracy: 0.9567\n\nEpoch 00003: val_loss did not improve from 0.13747\nEpoch 4/100\n77/77 [==============================] - 40s 512ms/step - loss: 0.0404 - accuracy: 0.9886 - val_loss: 1.9361 - val_accuracy: 0.7308\n\nEpoch 00004: val_loss did not improve from 0.13747\nEpoch 5/100\n77/77 [==============================] - 40s 519ms/step - loss: 0.0454 - accuracy: 0.9858 - val_loss: 0.2996 - val_accuracy: 0.9471\n\nEpoch 00005: val_loss did not improve from 0.13747\nEpoch 6/100\n77/77 [==============================] - 40s 515ms/step - loss: 0.0276 - accuracy: 0.9894 - val_loss: 0.3796 - val_accuracy: 0.8990\n\nEpoch 00006: val_loss did not improve from 0.13747\nEpoch 7/100\n77/77 [==============================] - 40s 516ms/step - loss: 0.0287 - accuracy: 0.9894 - val_loss: 0.1789 - val_accuracy: 0.9591\n\nEpoch 00007: val_loss did not improve from 0.13747\nEpoch 8/100\n77/77 [==============================] - 40s 521ms/step - loss: 0.0208 - accuracy: 0.9923 - val_loss: 0.2719 - val_accuracy: 0.9591\n\nEpoch 00008: val_loss did not improve from 0.13747\nEpoch 9/100\n77/77 [==============================] - 40s 514ms/step - loss: 0.0140 - accuracy: 0.9927 - val_loss: 0.3209 - val_accuracy: 0.9543\n\nEpoch 00009: val_loss did not improve from 0.13747\nEpoch 10/100\n77/77 [==============================] - 40s 518ms/step - loss: 0.0104 - accuracy: 0.9963 - val_loss: 0.2948 - val_accuracy: 0.9471\n\nEpoch 00010: val_loss did not improve from 0.13747\nEpoch 11/100\n77/77 [==============================] - 40s 518ms/step - loss: 0.0103 - accuracy: 0.9972 - val_loss: 0.2810 - val_accuracy: 0.9399\n\nEpoch 00011: val_loss did not improve from 0.13747\nEpoch 12/100\n77/77 [==============================] - 40s 518ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.2113 - val_accuracy: 0.9639\n\nEpoch 00012: val_loss did not improve from 0.13747\nEpoch 13/100\n77/77 [==============================] - 40s 520ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2798 - val_accuracy: 0.9663\n\nEpoch 00013: val_loss did not improve from 0.13747\nEpoch 14/100\n77/77 [==============================] - 40s 520ms/step - loss: 7.4748e-04 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9736\n\nEpoch 00014: val_loss did not improve from 0.13747\nEpoch 15/100\n77/77 [==============================] - 40s 517ms/step - loss: 0.0111 - accuracy: 0.9972 - val_loss: 0.4048 - val_accuracy: 0.9255\n\nEpoch 00015: val_loss did not improve from 0.13747\nEpoch 16/100\n77/77 [==============================] - 40s 524ms/step - loss: 0.0291 - accuracy: 0.9923 - val_loss: 0.2598 - val_accuracy: 0.9519\n\nEpoch 00016: val_loss did not improve from 0.13747\nEpoch 17/100\n77/77 [==============================] - 41s 525ms/step - loss: 0.0761 - accuracy: 0.9764 - val_loss: 0.2379 - val_accuracy: 0.9255\n\nEpoch 00017: val_loss did not improve from 0.13747\nEpoch 18/100\n77/77 [==============================] - 40s 515ms/step - loss: 0.0292 - accuracy: 0.9894 - val_loss: 0.2023 - val_accuracy: 0.9591\n\nEpoch 00018: val_loss did not improve from 0.13747\nEpoch 19/100\n77/77 [==============================] - 40s 514ms/step - loss: 0.0101 - accuracy: 0.9963 - val_loss: 0.2000 - val_accuracy: 0.9688\n\nEpoch 00019: val_loss did not improve from 0.13747\nEpoch 20/100\n77/77 [==============================] - 40s 515ms/step - loss: 0.0063 - accuracy: 0.9967 - val_loss: 0.1848 - val_accuracy: 0.9591\n\nEpoch 00020: val_loss did not improve from 0.13747\nEpoch 21/100\n77/77 [==============================] - 40s 519ms/step - loss: 0.0150 - accuracy: 0.9947 - val_loss: 0.4396 - val_accuracy: 0.9231\n\nEpoch 00021: val_loss did not improve from 0.13747\nEpoch 22/100\n77/77 [==============================] - 41s 525ms/step - loss: 0.0325 - accuracy: 0.9870 - val_loss: 0.2599 - val_accuracy: 0.9399\n\nEpoch 00022: val_loss did not improve from 0.13747\nEpoch 23/100\n77/77 [==============================] - 40s 521ms/step - loss: 0.0967 - accuracy: 0.9711 - val_loss: 0.3430 - val_accuracy: 0.9375\n\nEpoch 00023: val_loss did not improve from 0.13747\nEpoch 24/100\n77/77 [==============================] - 40s 524ms/step - loss: 0.0470 - accuracy: 0.9854 - val_loss: 0.3283 - val_accuracy: 0.9255\n\nEpoch 00024: val_loss did not improve from 0.13747\nEpoch 25/100\n77/77 [==============================] - 41s 528ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 0.4225 - val_accuracy: 0.9014\n\nEpoch 00025: val_loss did not improve from 0.13747\nEpoch 26/100\n77/77 [==============================] - 41s 530ms/step - loss: 0.0069 - accuracy: 0.9984 - val_loss: 0.2339 - val_accuracy: 0.9519\n\nEpoch 00026: val_loss did not improve from 0.13747\nEpoch 27/100\n77/77 [==============================] - 41s 525ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.4735 - val_accuracy: 0.9111\n\nEpoch 00027: val_loss did not improve from 0.13747\nEpoch 28/100\n77/77 [==============================] - 41s 528ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.3409 - val_accuracy: 0.9519\n\nEpoch 00028: val_loss did not improve from 0.13747\nEpoch 29/100\n77/77 [==============================] - 41s 531ms/step - loss: 0.0162 - accuracy: 0.9943 - val_loss: 0.2285 - val_accuracy: 0.9615\n\nEpoch 00029: val_loss did not improve from 0.13747\nEpoch 30/100\n77/77 [==============================] - 41s 530ms/step - loss: 0.0168 - accuracy: 0.9927 - val_loss: 0.2374 - val_accuracy: 0.9615\n\nEpoch 00030: val_loss did not improve from 0.13747\nEpoch 31/100\n77/77 [==============================] - 41s 530ms/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.2868 - val_accuracy: 0.9543\n\nEpoch 00031: val_loss did not improve from 0.13747\nEpoch 32/100\n77/77 [==============================] - 41s 533ms/step - loss: 0.0074 - accuracy: 0.9967 - val_loss: 0.3361 - val_accuracy: 0.9519\n\nEpoch 00032: val_loss did not improve from 0.13747\nEpoch 33/100\n77/77 [==============================] - 41s 533ms/step - loss: 0.0672 - accuracy: 0.9854 - val_loss: 3.6944 - val_accuracy: 0.6058\n\nEpoch 00033: val_loss did not improve from 0.13747\nEpoch 34/100\n77/77 [==============================] - 41s 534ms/step - loss: 0.0331 - accuracy: 0.9886 - val_loss: 0.2257 - val_accuracy: 0.9567\n\nEpoch 00034: val_loss did not improve from 0.13747\nEpoch 35/100\n77/77 [==============================] - 41s 533ms/step - loss: 0.0092 - accuracy: 0.9967 - val_loss: 0.2588 - val_accuracy: 0.9639\n\nEpoch 00035: val_loss did not improve from 0.13747\nEpoch 36/100\n77/77 [==============================] - 41s 534ms/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 0.5355 - val_accuracy: 0.9111\n\nEpoch 00036: val_loss did not improve from 0.13747\nEpoch 37/100\n77/77 [==============================] - 41s 533ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.4133 - val_accuracy: 0.9351\n\nEpoch 00037: val_loss did not improve from 0.13747\nEpoch 38/100\n77/77 [==============================] - 41s 537ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.2893 - val_accuracy: 0.9639\n\nEpoch 00038: val_loss did not improve from 0.13747\nEpoch 39/100\n77/77 [==============================] - 41s 537ms/step - loss: 6.6611e-04 - accuracy: 1.0000 - val_loss: 0.2798 - val_accuracy: 0.9712\n\nEpoch 00039: val_loss did not improve from 0.13747\nEpoch 40/100\n77/77 [==============================] - 41s 535ms/step - loss: 2.1870e-04 - accuracy: 1.0000 - val_loss: 0.2847 - val_accuracy: 0.9663\n\nEpoch 00040: val_loss did not improve from 0.13747\nEpoch 41/100\n77/77 [==============================] - 41s 536ms/step - loss: 5.4272e-04 - accuracy: 1.0000 - val_loss: 0.3340 - val_accuracy: 0.9736\n\nEpoch 00041: val_loss did not improve from 0.13747\nEpoch 42/100\n77/77 [==============================] - 41s 536ms/step - loss: 3.4648e-04 - accuracy: 1.0000 - val_loss: 0.3415 - val_accuracy: 0.9663\n\nEpoch 00042: val_loss did not improve from 0.13747\nEpoch 43/100\n77/77 [==============================] - 41s 537ms/step - loss: 1.8220e-04 - accuracy: 1.0000 - val_loss: 0.2818 - val_accuracy: 0.9688\n\nEpoch 00043: val_loss did not improve from 0.13747\nEpoch 44/100\n77/77 [==============================] - 41s 535ms/step - loss: 1.0875e-04 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 0.9688\n\nEpoch 00044: val_loss did not improve from 0.13747\nEpoch 45/100\n77/77 [==============================] - 42s 545ms/step - loss: 1.0832e-04 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 0.9688\n\nEpoch 00045: val_loss did not improve from 0.13747\nEpoch 46/100\n77/77 [==============================] - 41s 532ms/step - loss: 1.1757e-04 - accuracy: 1.0000 - val_loss: 0.3353 - val_accuracy: 0.9760\n\nEpoch 00046: val_loss did not improve from 0.13747\nEpoch 47/100\n77/77 [==============================] - 41s 534ms/step - loss: 9.9014e-05 - accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.9688\n\nEpoch 00047: val_loss did not improve from 0.13747\nEpoch 48/100\n77/77 [==============================] - 42s 540ms/step - loss: 5.7616e-05 - accuracy: 1.0000 - val_loss: 0.3502 - val_accuracy: 0.9688\n\nEpoch 00048: val_loss did not improve from 0.13747\nEpoch 49/100\n77/77 [==============================] - 42s 546ms/step - loss: 6.9429e-05 - accuracy: 1.0000 - val_loss: 0.3414 - val_accuracy: 0.9736\n\nEpoch 00049: val_loss did not improve from 0.13747\nEpoch 50/100\n77/77 [==============================] - 42s 539ms/step - loss: 3.6800e-05 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.9688\n\nEpoch 00050: val_loss did not improve from 0.13747\nEpoch 51/100\n77/77 [==============================] - 41s 535ms/step - loss: 8.5921e-05 - accuracy: 1.0000 - val_loss: 0.3638 - val_accuracy: 0.9688\n\nEpoch 00051: val_loss did not improve from 0.13747\nEpoch 52/100\n77/77 [==============================] - 42s 539ms/step - loss: 5.7755e-05 - accuracy: 1.0000 - val_loss: 0.2376 - val_accuracy: 0.9736\n\nEpoch 00052: val_loss did not improve from 0.13747\nEpoch 53/100\n77/77 [==============================] - 42s 544ms/step - loss: 5.6990e-05 - accuracy: 1.0000 - val_loss: 0.3546 - val_accuracy: 0.9712\n\nEpoch 00053: val_loss did not improve from 0.13747\nEpoch 54/100\n77/77 [==============================] - 41s 536ms/step - loss: 7.9608e-05 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.9688\n\nEpoch 00054: val_loss did not improve from 0.13747\nEpoch 55/100\n77/77 [==============================] - 41s 536ms/step - loss: 3.7803e-05 - accuracy: 1.0000 - val_loss: 0.3152 - val_accuracy: 0.9784\n\nEpoch 00055: val_loss did not improve from 0.13747\nEpoch 56/100\n77/77 [==============================] - 41s 535ms/step - loss: 2.4657e-05 - accuracy: 1.0000 - val_loss: 0.3735 - val_accuracy: 0.9688\n\nEpoch 00056: val_loss did not improve from 0.13747\nEpoch 57/100\n77/77 [==============================] - 42s 537ms/step - loss: 1.8678e-05 - accuracy: 1.0000 - val_loss: 0.3780 - val_accuracy: 0.9688\n\nEpoch 00057: val_loss did not improve from 0.13747\nEpoch 58/100\n77/77 [==============================] - 41s 534ms/step - loss: 1.9863e-05 - accuracy: 1.0000 - val_loss: 0.3765 - val_accuracy: 0.9688\n\nEpoch 00058: val_loss did not improve from 0.13747\nEpoch 59/100\n77/77 [==============================] - 41s 534ms/step - loss: 1.7586e-05 - accuracy: 1.0000 - val_loss: 0.3767 - val_accuracy: 0.9688\n\nEpoch 00059: val_loss did not improve from 0.13747\nEpoch 60/100\n77/77 [==============================] - 42s 538ms/step - loss: 2.2448e-04 - accuracy: 1.0000 - val_loss: 0.4374 - val_accuracy: 0.9471\n\nEpoch 00060: val_loss did not improve from 0.13747\nEpoch 61/100\n77/77 [==============================] - 41s 535ms/step - loss: 9.2929e-05 - accuracy: 1.0000 - val_loss: 0.2716 - val_accuracy: 0.9712\n\nEpoch 00061: val_loss did not improve from 0.13747\nEpoch 62/100\n77/77 [==============================] - 41s 534ms/step - loss: 6.5878e-05 - accuracy: 1.0000 - val_loss: 0.3772 - val_accuracy: 0.9688\n\nEpoch 00062: val_loss did not improve from 0.13747\nEpoch 63/100\n77/77 [==============================] - 41s 537ms/step - loss: 7.7029e-05 - accuracy: 1.0000 - val_loss: 0.3749 - val_accuracy: 0.9712\n\nEpoch 00063: val_loss did not improve from 0.13747\nEpoch 64/100\n77/77 [==============================] - 41s 537ms/step - loss: 2.7927e-05 - accuracy: 1.0000 - val_loss: 0.3808 - val_accuracy: 0.9688\n\nEpoch 00064: val_loss did not improve from 0.13747\nEpoch 65/100\n77/77 [==============================] - 41s 533ms/step - loss: 3.5027e-05 - accuracy: 1.0000 - val_loss: 0.3504 - val_accuracy: 0.9688\n\nEpoch 00065: val_loss did not improve from 0.13747\nEpoch 66/100\n77/77 [==============================] - 41s 532ms/step - loss: 1.4822e-05 - accuracy: 1.0000 - val_loss: 0.3283 - val_accuracy: 0.9712\n\nEpoch 00066: val_loss did not improve from 0.13747\nEpoch 67/100\n77/77 [==============================] - 41s 537ms/step - loss: 2.0083e-05 - accuracy: 1.0000 - val_loss: 0.3894 - val_accuracy: 0.9663\n\nEpoch 00067: val_loss did not improve from 0.13747\nEpoch 68/100\n77/77 [==============================] - 42s 539ms/step - loss: 1.2528e-05 - accuracy: 1.0000 - val_loss: 0.3823 - val_accuracy: 0.9712\n\nEpoch 00068: val_loss did not improve from 0.13747\nEpoch 69/100\n77/77 [==============================] - 42s 537ms/step - loss: 1.2874e-05 - accuracy: 1.0000 - val_loss: 0.3937 - val_accuracy: 0.9688\n\nEpoch 00069: val_loss did not improve from 0.13747\nEpoch 70/100\n77/77 [==============================] - 41s 537ms/step - loss: 1.5026e-05 - accuracy: 1.0000 - val_loss: 0.4004 - val_accuracy: 0.9688\n\nEpoch 00070: val_loss did not improve from 0.13747\nEpoch 71/100\n77/77 [==============================] - 42s 538ms/step - loss: 1.2867e-05 - accuracy: 1.0000 - val_loss: 0.4021 - val_accuracy: 0.9688\n\nEpoch 00071: val_loss did not improve from 0.13747\nEpoch 72/100\n77/77 [==============================] - 41s 537ms/step - loss: 1.0195e-05 - accuracy: 1.0000 - val_loss: 0.3414 - val_accuracy: 0.9736\n\nEpoch 00072: val_loss did not improve from 0.13747\nEpoch 73/100\n77/77 [==============================] - 41s 532ms/step - loss: 7.1273e-06 - accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 0.9688\n\nEpoch 00073: val_loss did not improve from 0.13747\nEpoch 74/100\n77/77 [==============================] - 41s 536ms/step - loss: 6.8693e-06 - accuracy: 1.0000 - val_loss: 0.3693 - val_accuracy: 0.9712\n\nEpoch 00074: val_loss did not improve from 0.13747\nEpoch 75/100\n77/77 [==============================] - 41s 536ms/step - loss: 4.6075e-06 - accuracy: 1.0000 - val_loss: 0.3982 - val_accuracy: 0.9712\n\nEpoch 00075: val_loss did not improve from 0.13747\nEpoch 76/100\n77/77 [==============================] - 42s 541ms/step - loss: 7.4342e-06 - accuracy: 1.0000 - val_loss: 0.4081 - val_accuracy: 0.9712\n\nEpoch 00076: val_loss did not improve from 0.13747\nEpoch 77/100\n77/77 [==============================] - 41s 531ms/step - loss: 4.6574e-06 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 0.9736\n\nEpoch 00077: val_loss did not improve from 0.13747\nEpoch 78/100\n77/77 [==============================] - 41s 531ms/step - loss: 3.9350e-06 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.9736\n\nEpoch 00078: val_loss did not improve from 0.13747\nEpoch 79/100\n77/77 [==============================] - 42s 538ms/step - loss: 5.0549e-06 - accuracy: 1.0000 - val_loss: 0.3907 - val_accuracy: 0.9760\n\nEpoch 00079: val_loss did not improve from 0.13747\nEpoch 80/100\n77/77 [==============================] - 41s 533ms/step - loss: 7.4599e-06 - accuracy: 1.0000 - val_loss: 0.3995 - val_accuracy: 0.9712\n\nEpoch 00080: val_loss did not improve from 0.13747\nEpoch 81/100\n77/77 [==============================] - 41s 532ms/step - loss: 2.7768e-06 - accuracy: 1.0000 - val_loss: 0.3576 - val_accuracy: 0.9760\n\nEpoch 00081: val_loss did not improve from 0.13747\nEpoch 82/100\n77/77 [==============================] - 41s 534ms/step - loss: 5.5954e-06 - accuracy: 1.0000 - val_loss: 0.4048 - val_accuracy: 0.9712\n\nEpoch 00082: val_loss did not improve from 0.13747\nEpoch 83/100\n77/77 [==============================] - 42s 538ms/step - loss: 6.2540e-06 - accuracy: 1.0000 - val_loss: 0.3189 - val_accuracy: 0.9760\n\nEpoch 00083: val_loss did not improve from 0.13747\nEpoch 84/100\n77/77 [==============================] - 41s 532ms/step - loss: 3.5881e-06 - accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 0.9712\n\nEpoch 00084: val_loss did not improve from 0.13747\nEpoch 85/100\n77/77 [==============================] - 41s 528ms/step - loss: 4.6147e-06 - accuracy: 1.0000 - val_loss: 0.3763 - val_accuracy: 0.9736\n\nEpoch 00085: val_loss did not improve from 0.13747\nEpoch 86/100\n77/77 [==============================] - 41s 532ms/step - loss: 5.8906e-06 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9712\n\nEpoch 00086: val_loss did not improve from 0.13747\nEpoch 87/100\n77/77 [==============================] - 41s 537ms/step - loss: 4.6145e-06 - accuracy: 1.0000 - val_loss: 0.3579 - val_accuracy: 0.9736\n\nEpoch 00087: val_loss did not improve from 0.13747\nEpoch 88/100\n77/77 [==============================] - 41s 533ms/step - loss: 0.5011 - accuracy: 0.9296 - val_loss: 15000.6299 - val_accuracy: 0.5385\n\nEpoch 00088: val_loss did not improve from 0.13747\nEpoch 89/100\n77/77 [==============================] - 41s 532ms/step - loss: 0.1686 - accuracy: 0.9386 - val_loss: 167.6303 - val_accuracy: 0.5409\n\nEpoch 00089: val_loss did not improve from 0.13747\nEpoch 90/100\n77/77 [==============================] - 41s 531ms/step - loss: 0.1020 - accuracy: 0.9634 - val_loss: 0.9720 - val_accuracy: 0.7139\n\nEpoch 00090: val_loss did not improve from 0.13747\nEpoch 91/100\n77/77 [==============================] - 41s 535ms/step - loss: 0.0850 - accuracy: 0.9719 - val_loss: 0.1601 - val_accuracy: 0.9471\n\nEpoch 00091: val_loss did not improve from 0.13747\nEpoch 92/100\n77/77 [==============================] - 41s 532ms/step - loss: 0.0674 - accuracy: 0.9793 - val_loss: 0.1623 - val_accuracy: 0.9351\n\nEpoch 00092: val_loss did not improve from 0.13747\nEpoch 93/100\n77/77 [==============================] - 41s 529ms/step - loss: 0.0593 - accuracy: 0.9780 - val_loss: 0.1574 - val_accuracy: 0.9567\n\nEpoch 00093: val_loss did not improve from 0.13747\nEpoch 94/100\n77/77 [==============================] - 41s 530ms/step - loss: 0.0462 - accuracy: 0.9813 - val_loss: 0.2519 - val_accuracy: 0.9279\n\nEpoch 00094: val_loss did not improve from 0.13747\nEpoch 95/100\n77/77 [==============================] - 42s 539ms/step - loss: 0.0442 - accuracy: 0.9858 - val_loss: 0.1940 - val_accuracy: 0.9495\n\nEpoch 00095: val_loss did not improve from 0.13747\nEpoch 96/100\n77/77 [==============================] - 41s 530ms/step - loss: 0.0264 - accuracy: 0.9910 - val_loss: 0.0916 - val_accuracy: 0.9808\n\nEpoch 00096: val_loss improved from 0.13747 to 0.09160, saving model to /kaggle/working/preprocess-binary-aptos-resnet-5-100-weights.hdf5\nEpoch 97/100\n77/77 [==============================] - 41s 529ms/step - loss: 0.0200 - accuracy: 0.9947 - val_loss: 0.1737 - val_accuracy: 0.9519\n\nEpoch 00097: val_loss did not improve from 0.09160\nEpoch 98/100\n77/77 [==============================] - 41s 534ms/step - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.1486 - val_accuracy: 0.9688\n\nEpoch 00098: val_loss did not improve from 0.09160\nEpoch 99/100\n77/77 [==============================] - 41s 534ms/step - loss: 0.0434 - accuracy: 0.9845 - val_loss: 0.2125 - val_accuracy: 0.9543\n\nEpoch 00099: val_loss did not improve from 0.09160\nEpoch 100/100\n77/77 [==============================] - 41s 531ms/step - loss: 0.0374 - accuracy: 0.9849 - val_loss: 0.2138 - val_accuracy: 0.9591\n\nEpoch 00100: val_loss did not improve from 0.09160\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('/kaggle/working/preprocess-binary-aptos-resnet-5-100-save-weights-2.hdf5')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:54:03.585958Z","iopub.execute_input":"2021-06-08T13:54:03.586338Z","iopub.status.idle":"2021-06-08T13:54:04.432209Z","shell.execute_reply.started":"2021-06-08T13:54:03.586308Z","shell.execute_reply":"2021-06-08T13:54:04.431023Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model.load_weights('/kaggle/working/preprocess-binary-aptos-resnet-5-100-weights.hdf5')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:59:09.258621Z","iopub.execute_input":"2021-06-08T13:59:09.259147Z","iopub.status.idle":"2021-06-08T13:59:09.551547Z","shell.execute_reply.started":"2021-06-08T13:59:09.259113Z","shell.execute_reply":"2021-06-08T13:59:09.550470Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train_loss','val_loss'], loc = 'upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:59:12.842739Z","iopub.execute_input":"2021-06-08T13:59:12.843256Z","iopub.status.idle":"2021-06-08T13:59:13.045949Z","shell.execute_reply.started":"2021-06-08T13:59:12.843190Z","shell.execute_reply":"2021-06-08T13:59:13.044928Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"evaluate = model.evaluate(test_generator, steps = test_generator.n // 32, verbose =1)\n\nprint('Accuracy Test : {}'.format(evaluate[1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:00:21.619673Z","iopub.execute_input":"2021-06-08T14:00:21.620153Z","iopub.status.idle":"2021-06-08T14:00:36.520744Z","shell.execute_reply.started":"2021-06-08T14:00:21.620120Z","shell.execute_reply":"2021-06-08T14:00:36.519529Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"22/22 [==============================] - 14s 632ms/step - loss: 0.0829 - accuracy: 0.9787\nAccuracy Test : 0.9786931872367859\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom PIL import Image\n\nprediction = []\noriginal = []\n# image = []\ncount = 0\n\nfor item in range(len(test)):\n# for item in test[]:\n    img = PIL.Image.open(path + test['Image'].tolist()[item])\n    # img = PIL.Image.open(path + test)\n    img = np.asarray(img, dtype=np.float32)\n    img = img / 255\n    img = img.reshape(-1, 512, 512, 3)\n    predict = model.predict(img)\n    predict = np.argmax(predict)\n    prediction.append(str(predict))\n    # prediction.append(predict)\n    original.append(test['Labels'].tolist()[item])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:00:40.923150Z","iopub.execute_input":"2021-06-08T14:00:40.923493Z","iopub.status.idle":"2021-06-08T14:01:21.386679Z","shell.execute_reply.started":"2021-06-08T14:00:40.923454Z","shell.execute_reply":"2021-06-08T14:01:21.385627Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(original[:5])\nprint(prediction[:5])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:02:24.105268Z","iopub.execute_input":"2021-06-08T14:02:24.105678Z","iopub.status.idle":"2021-06-08T14:02:24.112806Z","shell.execute_reply.started":"2021-06-08T14:02:24.105643Z","shell.execute_reply":"2021-06-08T14:02:24.111188Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"['0', '0', '1', '0', '0']\n['0', '0', '1', '0', '0']\n","output_type":"stream"}]},{"cell_type":"code","source":"score = accuracy_score(original, prediction)\nprint(\"Test Accuracy : {}\".format(score))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:02:27.286739Z","iopub.execute_input":"2021-06-08T14:02:27.287160Z","iopub.status.idle":"2021-06-08T14:02:27.296656Z","shell.execute_reply.started":"2021-06-08T14:02:27.287114Z","shell.execute_reply":"2021-06-08T14:02:27.294869Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Test Accuracy : 0.9768076398362893\n","output_type":"stream"}]},{"cell_type":"code","source":"print(classification_report(np.asarray(original), np.asarray(prediction)))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:02:40.576996Z","iopub.execute_input":"2021-06-08T14:02:40.577357Z","iopub.status.idle":"2021-06-08T14:02:40.601821Z","shell.execute_reply.started":"2021-06-08T14:02:40.577326Z","shell.execute_reply":"2021-06-08T14:02:40.600673Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.98       354\n           1       0.98      0.97      0.98       379\n\n    accuracy                           0.98       733\n   macro avg       0.98      0.98      0.98       733\nweighted avg       0.98      0.98      0.98       733\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_path = '../input/test-dataset/test-dataset-idrid-eye/'","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:26:22.376077Z","iopub.execute_input":"2021-06-08T14:26:22.376692Z","iopub.status.idle":"2021-06-08T14:26:22.384527Z","shell.execute_reply.started":"2021-06-08T14:26:22.376639Z","shell.execute_reply":"2021-06-08T14:26:22.383349Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"test_org = []\ntest_pred = []\n\nfor i in range(2):\n    test_imgs = os.listdir(test_path + str(i))\n    \n    for test in test_imgs:\n        test = Image.open(test_path + str(i) + '/' + test)\n        test = np.asarray(test, dtype='float32')\n        test = test / 255\n        test = test.reshape(-1, 512, 512, 3)\n        predict = model.predict(test)\n        predict = np.argmax(predict)\n        test_pred.append(str(predict))\n        test_org.append(str(i))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:31:15.771290Z","iopub.execute_input":"2021-06-08T14:31:15.771625Z","iopub.status.idle":"2021-06-08T14:31:27.783706Z","shell.execute_reply.started":"2021-06-08T14:31:15.771595Z","shell.execute_reply":"2021-06-08T14:31:27.782441Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"test_score = accuracy_score(test_org, test_pred)\nprint(\"Test Accuracy : {}\".format(test_score))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:34:40.453182Z","iopub.execute_input":"2021-06-08T14:34:40.453560Z","iopub.status.idle":"2021-06-08T14:34:40.460987Z","shell.execute_reply.started":"2021-06-08T14:34:40.453527Z","shell.execute_reply":"2021-06-08T14:34:40.459681Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Test Accuracy : 0.7098214285714286\n","output_type":"stream"}]},{"cell_type":"code","source":"print(classification_report(np.asarray(test_org), np.asarray(test_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T14:35:12.353217Z","iopub.execute_input":"2021-06-08T14:35:12.353573Z","iopub.status.idle":"2021-06-08T14:35:12.371517Z","shell.execute_reply.started":"2021-06-08T14:35:12.353541Z","shell.execute_reply":"2021-06-08T14:35:12.370141Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.98      0.47      0.63       120\n           1       0.62      0.99      0.76       104\n\n    accuracy                           0.71       224\n   macro avg       0.80      0.73      0.70       224\nweighted avg       0.81      0.71      0.69       224\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./preprocess-binary-aptos-resnet-5-100-weights.hdf5\"> Download File </a>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}